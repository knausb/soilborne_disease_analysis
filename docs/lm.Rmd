---
title: "The Linear Model"
---


The linear model is the foundation of statistical hypothesis testing.
Many of the analyses we encounter in the technical literature can be seen as extensions of the linear model.
Understanding experimental design requires an understanding of what test we plan to perform.
A foundation in the linear model will help us understand the design and implementation of our experiments.


## The linear equation


In grade school geometry we learned the equation of a line.

$$
y = mx + b
$$


Here, `x` and `y` are coordinates on the plane.
The slope is `m`.
And the y-intercept is `b`.
We can plot this in R as follows.


```{r, fig.align='center', fig.cap='Figure 1. A straight line.'}
x <- 1:20
y <- 0.5 * x + 2
plot(x, y)
```


The first line generates some data and assigns it to the variable `x`.
The second line uses a contrived slope (0.5) and intercept (2) to generate values which we assign to the variable `y`.
Because the values of `y` **depend** on `x` we sometimes call `y` the **dependent** variable and `x` the **independent** variable.
Similarly, we may refer to `y` as the **response** and `x` as **predictor(s)**.
The line crosses the y-intercept at one and for each increase of value in `x` we see a half increase in `y`.


## Adding the error term


In biology we rarely see such clear patterns.
In order to accomodate this variation we add an error term.

$$
y = mx + b + \epsilon_{Norm}
$$


This can be plotted in R as follows.


```{r, fig.align='center', fig.cap='Figure 2. A straight line with random error.'}
set.seed(9)
y <- 0.5 * x + 2 + rnorm(n=length(x), mean = 0, sd = 1)
plot(x, y)
```


This is the exact same plot as in Figure 1 except that the data are now distributed around the line with normally distributed error.
The mean of this distribution was zero because we do not want it to deviate from the line.
The standard deviation, or the dispersal around the line, was arbitrarily set to one.
In your experiments the standard deviation will represent biological variation combined with technical error assotiated with measuring the data.

### Linear regression

When we use linear regression we are attempting to infer the slope and intercept that we've specified above.
We can use the function `lm()` to perform the regression and it's `summary()` method to report the results.


```{r}
lm1 <- lm(y ~ x)
summary(lm1)
```


In order to perform the linear regression we've used the function `lm()` and saved it's results in the object `lm1`.
We summarized the object we created with the generic function `summary()`.
We see that the intercept is ```r lm1$coefficients[1]```, which should be close to two, and the slope is ```r lm1$coefficients[2]```, which should be close to 0.5.


## Analysis of variance


In the analysis of soilborne pathogens we are typically looking for differences among groups.
In linear regression we were looking for relationships among a (putatively) dependent variable and independent variables.
When we investigate soilborne pathogens we're typically looking for differences among groups of plots.
For example, we may be looking for differences among plots that were fumigated, or fertilized, or ammended in some other manner.
These are frequently analysis of variance (ANOVA) type questions.
ANOVA can be seen as a form of the lineaar model where instead of trying to infer a slope and intercept we're trying to infer the elevation, or y-intercept, for two or more groups.


```{r}
set.seed(99)
my_data <- cbind( rep(c('A', 'B', 'C'), each = 10), rnorm(n = 30, mean = 10, sd = 2) )
my_data <- as.data.frame(my_data)
colnames(my_data) <- c("Group", "Value")
my_data$Group <- as.factor(my_data$Group)
my_data$Value <- as.numeric(my_data$Value)
```


```{r, fig.align='center', fig.cap="Figure 3. Box and whisker plot of simulated data with normal error."}
library(ggplot2)
p <- ggplot(data = my_data, mapping = aes(x = Group, y = Value))
p <- p + geom_boxplot()
p <- p + geom_jitter(shape=16, position=position_jitter(0.2))
p <- p + theme_bw()
p
```

Here we've simulated three groups of data.
We've visualized this data using box and whisker plots.
I like to add the dots as long as there aren't too many.
And I prefer `theme_bw()` over the default theme.
You may want to omit these steps if you like.

Note that we've created a **factor** to indicate which group each sample belongs to.
Note that a **factor** is different from a **vector of characters** because a factor includes information about the groups or levels and a vector of characters does not.


```{r}
my_data$Group
as.character(my_data$Group)
```


We can now test for differences in group means.

```{r}
options(contrasts = c("contr.helmert", "contr.poly"))
lm2 <- lm(Value ~ Group, data = my_data)
summary(lm2)
```


```{r}
# http://www.sthda.com/english/wiki/one-way-anova-test-in-r
library(dplyr)
group_by(my_data, Group) %>%
  summarise(
    count = n(),
    mean = mean(Value, na.rm = TRUE),
    sd = sd(Value, na.rm = TRUE)
  )
mean(my_data$Value)
```


Shapiro-Wilk test of normality

Levene's test for common variance

### Contrasts

```{r}
options("contrasts")

options(contrasts = c("contr.helmert", "contr.poly"))
options(contrasts = c("contr.treatment", "contr.poly"))
```


